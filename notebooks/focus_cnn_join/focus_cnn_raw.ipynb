{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/focus-convolutional-neural-network/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils.data_loaders import FocusCNNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FocusCNNLoader(\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    images_dir = \"/home/ubuntu/focus-convolutional-neural-network/data/processed/COCO-2017/CocoFocusCNN/train/images\",\n",
    "    csv_path=\"/home/ubuntu/focus-convolutional-neural-network/data/processed/COCO-2017/CocoFocusCNN/train/labels.csv\",\n",
    "    labels={\n",
    "        \"0\": \"none\",\n",
    "        \"1\": \"person\",\n",
    "        \"2\": \"car\",\n",
    "        \"3\": \"bicycle\"\n",
    "    },\n",
    "    tf_image_size=(640, 640),\n",
    "    save_out_dir=\"/home/ubuntu/focus-convolutional-neural-network/res/focus_cnn/coco_focuscnn/trainer/labels/\",\n",
    "    validation_split=0.15,\n",
    "    is_test=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = loader.get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "data = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'label': tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.]),\n",
       "  'transform': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.0314e-01, -5.3048e-01, -1.5887e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 9.2952e-01, -4.8666e-01, -5.2706e+00],\n",
       "          [ 1.6430e-01, -4.9750e-02, -6.4370e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 6.8483e-01,  3.4219e-03, -2.1521e+00],\n",
       "          [ 2.2016e-02,  1.1886e-01, -1.4836e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 1.9833e-01,  6.5637e-01, -2.7717e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 6.0781e-02,  8.1197e-01, -3.0236e+00]])},\n",
       " '2': {'label': tensor([0., 0., 0., 0., 2., 2., 2., 0., 0., 0., 2., 0., 0., 0., 0., 0.]),\n",
       "  'transform': tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.9090, -0.0556, -3.5739],\n",
       "          [ 0.0551, -0.4209, -2.0351],\n",
       "          [-0.4696, -0.1900, -0.6016],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0698, -0.0727, -3.6648],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]])},\n",
       " '3': {'label': tensor([0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'transform': tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0231,  0.1380, -0.8003],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]])}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from models import ResNet34Classifier, ResFocusNetwork, TransformModule\n",
    "from utils import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cls_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model = ResNet34Classifier(n_classes=4)\n",
    "model_checkpoint = torch.load(\"/home/ubuntu/focus-convolutional-neural-network/res/classifiers/coco_classifier_multi/trainer/0116_044824/models/checkpoint-epoch44.pth\")\n",
    "\n",
    "cls_model.load_state_dict(model_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_person_model = ResFocusNetwork(threshold=0.5, backbone=\"resnet34\", loss_lambda_tr=1,loss_lambda_sc=1, loss_lambda_rot=3, inp_img_size=(640, 640), loss_rot=False)\n",
    "model_checkpoint = torch.load(\"/home/ubuntu/focus-convolutional-neural-network/res/focus/coco_focus_person/trainer/0103_092318/models/checkpoint-epoch79.pth\")\n",
    "\n",
    "focus_person_model.load_state_dict(model_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_car_model = ResFocusNetwork(threshold=0.5, backbone=\"resnet34\", loss_lambda_tr=1,loss_lambda_sc=1, loss_lambda_rot=3, inp_img_size=(640, 640), loss_rot=False)\n",
    "model_checkpoint = torch.load(\"/home/ubuntu/focus-convolutional-neural-network/res/focus/coco_focus_car/trainer/0111_050401/models/checkpoint-epoch19.pth\")\n",
    "\n",
    "focus_car_model.load_state_dict(model_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_bicycle_model = ResFocusNetwork(threshold=0.5, backbone=\"resnet34\", loss_lambda_tr=1,loss_lambda_sc=1, loss_lambda_rot=3, inp_img_size=(640, 640), loss_rot=False)\n",
    "model_checkpoint = torch.load(\"/home/ubuntu/focus-convolutional-neural-network/res/focus/coco_focus_bicycle/trainer/0114_175807/models/checkpoint-epoch56.pth\")\n",
    "\n",
    "focus_bicycle_model.load_state_dict(model_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transform.TransformModule import TransformModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_module = TransformModule(image_out_sz=(300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = loader.get_train_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_model.eval()\n",
    "# focus_person_model.eval()\n",
    "# focus_car_model.eval()\n",
    "# focus_bicycle_model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     data = next(iter(dl_train))\n",
    "#     focus_person_model.to(device=\"cuda\")\n",
    "#     focus_car_model.to(device=\"cuda\")\n",
    "#     cls_model.to(device=\"cuda\")\n",
    "#     inp_img = data[0].to(device=\"cuda\")\n",
    "#     for k,v in data[2]['1'].items():\n",
    "#         data[2]['1'][k] = v.to(device=\"cuda\")\n",
    "#         data[2]['2'][k] = v.to(device=\"cuda\")\n",
    "\n",
    "#     out_focus_person = focus_person_model(inp_img, data[2]['1'])\n",
    "#     out_focus_car = focus_car_model(inp_img, data[2]['2'])\n",
    "\n",
    "#     out_focus_person_cat = torch.cat([out_focus_person['out_translate_x'], out_focus_person['out_translate_y'], out_focus_person['out_scale'], out_focus_person['out_rotate']],dim = 1)\n",
    "    \n",
    "#     out_focus_car_cat = torch.cat([out_focus_car['out_translate_x'], out_focus_car['out_translate_y'], out_focus_car['out_scale'], out_focus_car['out_rotate']],dim = 1)\n",
    "    \n",
    "#     inp_cls_person = tf_module(inp_img, out_focus_person_cat)\n",
    "#     inp_cls_car = tf_module(inp_img, out_focus_car_cat)\n",
    "    \n",
    "#     target_cls = torch.cat([data[2]['1']['label'],data[2]['2']['label']])\n",
    "    \n",
    "#     inp_cls = torch.cat([inp_cls_person, inp_cls_car])\n",
    "    \n",
    "#     out_cls = cls_model(inp_cls)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl_train)\n",
    "bn_1 = next(iter_dl)\n",
    "bn_2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/focus-convolutional-neural-network/src/pipeline/loss.py:112: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  * target_cls.T\n"
     ]
    }
   ],
   "source": [
    "cls_model.train()\n",
    "focus_person_model.train()\n",
    "focus_car_model.train()\n",
    "focus_bicycle_model.train()\n",
    "\n",
    "data = bn_2\n",
    "focus_person_model.to(device=\"cuda\")\n",
    "focus_car_model.to(device=\"cuda\")\n",
    "focus_bicycle_model.to(device=\"cuda\")\n",
    "cls_model.to(device=\"cuda\")\n",
    "inp_img = data[0].to(device=\"cuda\")\n",
    "for k in data[2]['1'].keys():\n",
    "    data[2]['1'][k] = data[2]['1'][k].to(device=\"cuda\")\n",
    "    data[2]['2'][k] = data[2]['2'][k].to(device=\"cuda\")\n",
    "    data[2]['3'][k] = data[2]['3'][k].to(device=\"cuda\")\n",
    "\n",
    "out_focus_person = focus_person_model(inp_img, data[2]['1'])\n",
    "out_focus_car = focus_car_model(inp_img, data[2]['2'])\n",
    "out_focus_bicycle = focus_bicycle_model(inp_img, data[2]['3'])\n",
    "\n",
    "out_focus_person_cat = torch.cat([out_focus_person['out_translate_x'], out_focus_person['out_translate_y'], out_focus_person['out_scale'], out_focus_person['out_rotate']],dim = 1)\n",
    "out_focus_car_cat = torch.cat([out_focus_car['out_translate_x'], out_focus_car['out_translate_y'], out_focus_car['out_scale'], out_focus_car['out_rotate']],dim = 1)\n",
    "out_focus_bicycle_cat = torch.cat([out_focus_bicycle['out_translate_x'], out_focus_bicycle['out_translate_y'], out_focus_bicycle['out_scale'], out_focus_bicycle['out_rotate']],dim = 1)\n",
    "    \n",
    "inp_cls_person = tf_module(inp_img, out_focus_person_cat)\n",
    "inp_cls_car = tf_module(inp_img, out_focus_car_cat)\n",
    "inp_cls_bicycle = tf_module(inp_img, out_focus_bicycle_cat)\n",
    "    \n",
    "target_cls = torch.cat([data[2]['1']['label'],data[2]['2']['label'],data[2]['3']['label']]).type(torch.LongTensor).to(device=\"cuda\")\n",
    "    \n",
    "inp_cls = torch.cat([inp_cls_person, inp_cls_car,inp_cls_bicycle])\n",
    "    \n",
    "out_cls = cls_model(inp_cls)\n",
    "\n",
    "loss_val = loss(out_cls, target_cls)\n",
    "\n",
    "loss_val.backward()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9681, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.7316e-01, 7.8600e-01, 1.4300e-02, 1.4229e-02],\n",
       "        [4.7360e-10, 1.0000e+00, 3.2201e-10, 8.4702e-12],\n",
       "        [9.9999e-01, 6.0839e-03, 1.0791e-04, 3.1950e-04],\n",
       "        [6.6723e-08, 1.0000e+00, 2.8146e-10, 1.4501e-10],\n",
       "        [1.0000e+00, 2.8351e-03, 3.6885e-04, 1.7107e-04],\n",
       "        [8.2177e-01, 9.6585e-01, 2.1397e-02, 6.0461e-03],\n",
       "        [9.9991e-01, 3.3740e-03, 2.1063e-03, 3.1507e-03],\n",
       "        [8.7184e-03, 9.9955e-01, 4.0898e-03, 4.8187e-03],\n",
       "        [1.0000e+00, 8.6415e-04, 7.1371e-04, 1.1578e-03],\n",
       "        [2.7567e-12, 1.0000e+00, 1.7300e-16, 1.7821e-14],\n",
       "        [7.1782e-07, 9.9999e-01, 1.3230e-05, 1.1679e-03],\n",
       "        [9.5203e-01, 6.7700e-01, 2.1367e-02, 5.1367e-03],\n",
       "        [1.0000e+00, 1.1474e-04, 1.1643e-03, 1.6977e-03],\n",
       "        [7.7516e-01, 7.2998e-01, 7.2511e-02, 2.7965e-02],\n",
       "        [5.8405e-01, 9.9958e-01, 1.6087e-04, 3.5766e-04],\n",
       "        [3.0637e-10, 1.0000e+00, 3.2563e-08, 7.9242e-12],\n",
       "        [9.9469e-01, 1.8517e-02, 1.5511e-01, 3.8700e-03],\n",
       "        [1.8851e-07, 1.0000e+00, 1.1547e-12, 3.2860e-10],\n",
       "        [9.9975e-01, 2.2630e-02, 4.2842e-03, 1.2063e-02],\n",
       "        [9.9993e-01, 4.5168e-02, 1.7023e-04, 6.1772e-03],\n",
       "        [9.9966e-01, 6.8204e-03, 5.5693e-03, 6.4204e-03],\n",
       "        [1.0000e+00, 7.0185e-05, 4.5935e-05, 5.0978e-06],\n",
       "        [1.0000e+00, 2.7295e-04, 1.8141e-05, 4.4039e-05],\n",
       "        [9.9877e-01, 1.8770e-03, 5.0016e-04, 1.8298e-02],\n",
       "        [1.0000e+00, 6.5897e-05, 1.0080e-04, 3.1292e-06],\n",
       "        [1.0000e+00, 1.8175e-03, 7.3549e-04, 3.3946e-04],\n",
       "        [9.8422e-01, 7.7242e-01, 5.0870e-03, 3.0228e-03],\n",
       "        [9.6221e-03, 9.9996e-01, 3.0887e-04, 3.3858e-04],\n",
       "        [1.0000e+00, 5.0407e-05, 4.1537e-05, 5.6096e-06],\n",
       "        [8.5096e-01, 8.4824e-01, 1.0621e-02, 4.5595e-03],\n",
       "        [9.9994e-01, 5.2805e-05, 3.5193e-01, 8.5384e-05],\n",
       "        [9.9999e-01, 8.0462e-03, 1.8898e-03, 7.6755e-04],\n",
       "        [2.3941e-02, 9.9992e-01, 1.3714e-03, 1.2760e-03],\n",
       "        [9.9954e-01, 3.3325e-03, 3.3460e-02, 4.6779e-03],\n",
       "        [1.0000e+00, 1.2324e-04, 5.5491e-04, 3.7908e-04],\n",
       "        [9.9047e-01, 1.4317e-01, 1.5795e-02, 1.6842e-02],\n",
       "        [9.9996e-01, 3.2467e-02, 9.7527e-04, 6.6969e-03],\n",
       "        [9.9467e-01, 8.2407e-01, 2.8806e-03, 1.2935e-03],\n",
       "        [1.0000e+00, 5.2519e-05, 2.0951e-06, 4.7707e-06],\n",
       "        [3.4857e-02, 9.9917e-01, 4.3287e-03, 2.5063e-03],\n",
       "        [9.9988e-01, 7.2896e-03, 4.9616e-02, 3.7093e-03],\n",
       "        [1.0000e+00, 2.4219e-05, 2.3191e-06, 1.1159e-05],\n",
       "        [9.9955e-01, 3.6096e-02, 8.9878e-03, 4.6752e-02],\n",
       "        [1.0000e+00, 4.7599e-04, 3.1016e-04, 1.0992e-05],\n",
       "        [9.9918e-01, 2.8751e-01, 2.2399e-03, 2.9795e-04],\n",
       "        [9.9997e-01, 4.8571e-04, 6.8344e-04, 2.8032e-03],\n",
       "        [7.8991e-05, 2.8802e-07, 3.4529e-08, 9.9999e-01],\n",
       "        [1.0000e+00, 1.6754e-06, 1.3593e-03, 2.6654e-04]], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out_cls, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c23f8bac5f54f555216c25507ea8382a3bcc21e21d40d6abf1fac4c9e8531714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
